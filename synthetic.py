import numpy as np
import pandas as pd
import random
from tensorflow.keras.layers import Dense, Input, LeakyReLU, BatchNormalization
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam

# Define a GAN architecture
def build_gan(input_dim, output_dim):
    # Generator
    generator_input = Input(shape=(input_dim,))
    generator = Dense(128)(generator_input)
    generator = LeakyReLU(0.2)(generator)
    generator = BatchNormalization()(generator)
    generator = Dense(output_dim, activation='tanh')(generator)  # Output layer with tanh activation
    generator = Model(generator_input, generator)

    # Discriminator
    discriminator_input = Input(shape=(output_dim,))
    discriminator = Dense(128)(discriminator_input)
    discriminator = LeakyReLU(0.2)(discriminator)
    discriminator = Dense(1, activation='sigmoid')(discriminator)
    discriminator = Model(discriminator_input, discriminator)
    discriminator.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5), metrics=['accuracy'])

    # GAN
    gan_input = Input(shape=(input_dim,))
    gan_output = discriminator(generator(gan_input))
    gan = Model(gan_input, gan_output)
    gan.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))

    return generator, discriminator, gan

# Define a function to generate synthetic data using GAN
def generate_synthetic_data(generator, num_samples=1000):
    noise = np.random.normal(0, 1, (num_samples, input_dim))
    synthetic_data = generator.predict(noise)
    return synthetic_data

# Example usage
if __name__ == "__main__":
    # Sample numerical data dimensions (customize for your data)
    num_samples = 1000
    num_features = 5
    input_dim = 100  # Size of the random noise vector for the generator

    # Build and train the GAN
    generator, discriminator, gan = build_gan(input_dim, num_features)

    # Train the GAN on your real data (replace with your data loading and preprocessing)
    real_data = pd.read_csv("your_real_data.csv")  # Load your real data
    real_data = real_data.to_numpy()  # Convert to NumPy array if not already
    real_data = (real_data - real_data.min()) / (real_data.max() - real_data.min())  # Normalize to [0, 1]

    epochs = 10000  # Number of training epochs (customize as needed)
    batch_size = 64  # Batch size for training

    for epoch in range(epochs):
        # Train the discriminator (real data vs. generated data)
        real_data_batch = real_data[np.random.randint(0, real_data.shape[0], batch_size)]
        fake_data_batch = generate_synthetic_data(generator, num_samples=batch_size)
        discriminator_loss_real = discriminator.train_on_batch(real_data_batch, np.ones((batch_size, 1)))
        discriminator_loss_fake = discriminator.train_on_batch(fake_data_batch, np.zeros((batch_size, 1)))
        discriminator_loss = 0.5 * np.add(discriminator_loss_real, discriminator_loss_fake)

        # Train the generator
        noise = np.random.normal(0, 1, (batch_size, input_dim))
        generator_loss = gan.train_on_batch(noise, np.ones((batch_size, 1)))

        # Print progress
        if epoch % 100 == 0:
            print(f"Epoch {epoch}/{epochs}, D Loss: {discriminator_loss[0]}, G Loss: {generator_loss}")

    # Generate synthetic data
    synthetic_data = generate_synthetic_data(generator, num_samples=num_samples)

    # Now, 'synthetic_data' contains synthetic data generated by the GAN that should be closer to real data.
